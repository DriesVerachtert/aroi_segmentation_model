{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11afda5a-abaf-4cda-9e06-f14f55269016",
   "metadata": {},
   "source": [
    "# Fine-tuning of a model for segmentation of retinal optical coherence tomography images (AROI)\n",
    "\n",
    "For more info, check the README.md file.\n",
    "\n",
    "## Fine-tune the model on the dataset\n",
    "\n",
    "This notebook fine-tunes some model, by training on the dataset that was created in 02_create_huggingface_dataset.ipynb.\n",
    "\n",
    "## Citations\n",
    "\n",
    "Information about the dataset can be found in the following publications:\n",
    "\n",
    "M. Melinščak, M. Radmilović, Z. Vatavuk, and S. Lončarić, \"Annotated retinal optical coherence tomography images (AROI) database for joint retinal layer and fluid segmentation,\" Automatika, vol. 62, no. 3, pp. 375-385, Jul. 2021. doi: 10.1080/00051144.2021.1973298\n",
    "\n",
    "M. Melinščak, M. Radmilović, Z. Vatavuk, and S. Lončarić, \"AROI: Annotated Retinal OCT Images database,\" in 2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO), Sep. 2021, pp. 400-405.\n",
    "\n",
    "M. Melinščak, \"Attention-based U-net: Joint segmentation of layers and fluids from retinal OCT images,\" in 2023 46th International Convention on Information, Communication and Electronic Technology (MIPRO), Sep. 2021, pp. 391-396."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4527f6a0-ba75-42a6-a661-fcbb16cdbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import io\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple, cast\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from transformers import AutoImageProcessor, DetrForSegmentation, SegformerImageProcessor\n",
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "from transformers.image_transforms import rgb_to_id\n",
    "from transformers import EvalPrediction\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217f7e5-07ff-4bdd-8a56-64c868cb97d7",
   "metadata": {},
   "source": [
    "First, load the dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a7a681-ae5b-4156-a5d2-1009f5ca50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_path: str = \"hf_aroi_dataset_split\"\n",
    "split_dataset: datasets.DatasetDict = datasets.load_from_disk(split_dataset_path)\n",
    "\n",
    "test_dataset: datasets.Dataset = split_dataset['test']\n",
    "train_dataset: datasets.Dataset = split_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d555349-74d7-4c17-9c6d-357d4bd239f0",
   "metadata": {},
   "source": [
    "Check if everything looks fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3086672-febc-41b4-97be-2156714b9a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset length: 22\n",
      "Train dataset length: 1115\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test dataset length: {len(test_dataset)}\")\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c8437-42b7-42d1-8d4f-0cc84b56f51a",
   "metadata": {},
   "source": [
    "Create an evaluation method. This is almost 100% based on some examples that I found, which is clearly reused everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8570b3f-741d-4741-bbef-8b0cb22add66",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric: evaluate.EvaluationModule = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_prediction: EvalPrediction) -> Dict:\n",
    "    with torch.no_grad():\n",
    "        logits: np.ndarray\n",
    "        labels: np.ndarray\n",
    "        logits, labels = cast(Tuple[np.ndarray, np.ndarray], eval_prediction)\n",
    "        logits_tensor: torch.Tensor = torch.from_numpy(logits)\n",
    "        interpolation_result: torch.Tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        logits_tensor = interpolation_result.argmax(dim=1)\n",
    "        pred_labels: np.ndarray = logits_tensor.detach().cpu().numpy()\n",
    "        metrics: Dict = cast(Dict, metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        ))\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1945b9b-6efc-4892-b01b-e76a7f52c27b",
   "metadata": {},
   "source": [
    "The trainer requires the mappings between IDs and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764c31f3-4105-4584-a06d-30abc9d32f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_short: List[str] = [\n",
    "    'above ILM',\n",
    "    'ILM-IPL/INL',\n",
    "    'IPL/INL-RPE',\n",
    "    'RPE-BM',\n",
    "    'under BM',\n",
    "    'PED',\n",
    "    'SRF',\n",
    "    'IRF',\n",
    "]\n",
    "id2label: Dict[int,str] = {v: k for v, k in enumerate(annotations_short)}\n",
    "label2id: Dict[str,int] = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57a314-802c-4a50-a1d3-f624d26eeefe",
   "metadata": {},
   "source": [
    "reduce_labels has to be False as there's no 'background' or unlabeled part in the images. Most examples have this set to True but it doesn't work for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44b3fc5-ca6e-4071-8693-74d6df43ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(model_name: str, output_dir: str, save_dir: str, num_epochs: int,\n",
    "                       batch_size: int, learning_rate: float, save_steps: int, eval_steps: int,\n",
    "                       logging_steps: int = 10, warmup_steps: int = 0):\n",
    "    \"\"\" Finetune a model on the dataset.\n",
    "    Note that this method has a side effect: it sets transformers on the test and train datasets \"\"\"\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    feature_extractor: SegformerImageProcessor = AutoImageProcessor.from_pretrained(model_name, reduce_labels=False)\n",
    "\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "        model_name, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    def transform_apply_feature_extractor(some_batch: Dict):\n",
    "        images: List = some_batch[\"image\"]\n",
    "        labels: List = some_batch[\"label\"]\n",
    "        inputs = feature_extractor(images, labels)\n",
    "        return inputs\n",
    "\n",
    "    test_dataset.set_transform(transform_apply_feature_extractor)\n",
    "    train_dataset.set_transform(transform_apply_feature_extractor)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_accumulation_steps=5,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_dir, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21ecd0-390b-4978-b9f0-fa1b79bd0fb0",
   "metadata": {},
   "source": [
    "For information about the model, see https://huggingface.co/nvidia/mit-b0\n",
    "\n",
    "Note that this is the most basic model out of five: b0 up to b5. I prefer to test first with the simplest one instead of immediately testing with the most complicated one.\n",
    "\n",
    "The model is stored locally in the directory 'nvidia-mit-b0-finetuned'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c417d9-fb0d-472a-9d8c-44098eb25001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: nvidia/mit-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/segformer/image_processing_segformer.py:103: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 2:21:11, Epoch 150/150]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mean Iou</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Per Category Iou</th>\n",
       "      <th>Per Category Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.983800</td>\n",
       "      <td>0.187288</td>\n",
       "      <td>0.471394</td>\n",
       "      <td>0.519274</td>\n",
       "      <td>0.948794</td>\n",
       "      <td>[0.9932763332658823, 0.8266048427335162, 0.7161208048998536, 0.12662054048058125, 0.9641020128888943, 0.1440327122183337, 0.00039413256563164027, 0.0]</td>\n",
       "      <td>[0.998351667279345, 0.9016869123887179, 0.9509127726269901, 0.1492644655116051, 0.99579295174979, 0.15779111568107002, 0.00039413256563164027, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.072426</td>\n",
       "      <td>0.693461</td>\n",
       "      <td>0.749050</td>\n",
       "      <td>0.977171</td>\n",
       "      <td>[0.9966487228700531, 0.9089897406856224, 0.8833012834846384, 0.3821064971925711, 0.9861019756830381, 0.6231685580383871, 0.7673708087089358, 0.0]</td>\n",
       "      <td>[0.998791946245383, 0.957102643037413, 0.9605953691545193, 0.5061131088591043, 0.9958171193369019, 0.7217771826203401, 0.8522002878881348, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.048844</td>\n",
       "      <td>0.756684</td>\n",
       "      <td>0.814381</td>\n",
       "      <td>0.983180</td>\n",
       "      <td>[0.9971643168761568, 0.9288407474780883, 0.9083248041074514, 0.47318955672525365, 0.9916985679879228, 0.7430508118333246, 0.8502893671161037, 0.16091219096334186]</td>\n",
       "      <td>[0.9981388421842933, 0.9735800260698338, 0.9573877327249656, 0.589228506047728, 0.9949625685613678, 0.9017272850458723, 0.9390979505106587, 0.16092362344582592]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>0.818605</td>\n",
       "      <td>0.871808</td>\n",
       "      <td>0.985694</td>\n",
       "      <td>[0.9973876493605115, 0.9362427820606157, 0.9255235051362367, 0.4940775968685851, 0.9928121165889119, 0.7795170139538903, 0.8539395382051892, 0.5693430656934306]</td>\n",
       "      <td>[0.998493707974712, 0.9763499459189616, 0.9628015863541708, 0.6333932657731285, 0.9976746249775789, 0.8629570291016045, 0.9221159777914867, 0.6206749555950266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.035962</td>\n",
       "      <td>0.835415</td>\n",
       "      <td>0.894026</td>\n",
       "      <td>0.986498</td>\n",
       "      <td>[0.9970542956814854, 0.9358839182607822, 0.9308055899386235, 0.5038223938223938, 0.9932834059112945, 0.797467434815143, 0.8748691521056445, 0.6501307034220533]</td>\n",
       "      <td>[0.9994063548346452, 0.9692986105333222, 0.9625164139906888, 0.6398659692710036, 0.997766763903443, 0.8749531712060706, 0.9309239838234286, 0.7774777975133215]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.033754</td>\n",
       "      <td>0.833021</td>\n",
       "      <td>0.883552</td>\n",
       "      <td>0.986933</td>\n",
       "      <td>[0.9967991970163573, 0.9367868613622066, 0.9316969164048627, 0.5133386176696078, 0.9940634706563954, 0.8155475863654984, 0.8723217336131582, 0.6036100612537469]</td>\n",
       "      <td>[0.9972611156282045, 0.9808046981168705, 0.9650343312155417, 0.6457175547564563, 0.9981122848754331, 0.9045409981109065, 0.9188258276783878, 0.6581172291296625]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.032336</td>\n",
       "      <td>0.835293</td>\n",
       "      <td>0.884029</td>\n",
       "      <td>0.987362</td>\n",
       "      <td>[0.9974259531258473, 0.9426469156688787, 0.9331645323692391, 0.5168734779079207, 0.9944649495111094, 0.8158982396034552, 0.8652208730327346, 0.6166503428011754]</td>\n",
       "      <td>[0.998213873647959, 0.9735384252710986, 0.976852404511471, 0.6556554429552142, 0.9976006117420487, 0.8921702256550053, 0.907224621290013, 0.6709769094138543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>0.842372</td>\n",
       "      <td>0.894151</td>\n",
       "      <td>0.987522</td>\n",
       "      <td>[0.9976147422130289, 0.9419181596727184, 0.9322581607813546, 0.5118614429172078, 0.9944356090926052, 0.8229847861749702, 0.8728251930922166, 0.665076575445644]</td>\n",
       "      <td>[0.9986753879339024, 0.9825623318634384, 0.9668315802970214, 0.6506864988558353, 0.9983173317473354, 0.8757263444845644, 0.9275824251148125, 0.7528241563055063]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.848709</td>\n",
       "      <td>0.898904</td>\n",
       "      <td>0.988234</td>\n",
       "      <td>[0.997558186957352, 0.941927664343051, 0.9372699608136481, 0.5299519712883306, 0.9951814833980749, 0.835041557211145, 0.8808006261880801, 0.6719437692964593]</td>\n",
       "      <td>[0.9984309458069917, 0.9645145186787586, 0.9750087320219826, 0.6564890487087284, 0.9978117005107291, 0.927353595255745, 0.9448728494070875, 0.7267495559502665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.029385</td>\n",
       "      <td>0.851711</td>\n",
       "      <td>0.906861</td>\n",
       "      <td>0.988424</td>\n",
       "      <td>[0.9976898007129046, 0.9459028251948701, 0.9387889448632428, 0.5354398288747458, 0.9949302294577695, 0.8358938396026576, 0.8799413193677635, 0.6851039839639188]</td>\n",
       "      <td>[0.998796665205362, 0.9701098261086613, 0.9709875806330385, 0.6628146453089245, 0.9976247793291606, 0.9215906645304766, 0.9559085612447734, 0.7770515097690941]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.028738</td>\n",
       "      <td>0.855517</td>\n",
       "      <td>0.909220</td>\n",
       "      <td>0.988603</td>\n",
       "      <td>[0.9976635668195379, 0.945542881425689, 0.9389393877634207, 0.5383136536637959, 0.9952451852038782, 0.842082515310458, 0.8818796943796944, 0.7044703535133113]</td>\n",
       "      <td>[0.9988400796371686, 0.9796433424855091, 0.9675190888632454, 0.6773618829682903, 0.9978075467066942, 0.9162581601664316, 0.93356295839331, 0.8027708703374778]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.035367</td>\n",
       "      <td>0.843015</td>\n",
       "      <td>0.890480</td>\n",
       "      <td>0.987212</td>\n",
       "      <td>[0.9974152517594774, 0.9409789256254134, 0.9358775041405174, 0.5095959661763365, 0.9927082329028751, 0.8123949241150595, 0.8785422537441029, 0.6766047677183062]</td>\n",
       "      <td>[0.9980765519125708, 0.9665148237512827, 0.9781014152507527, 0.6392938868911409, 0.9992436300471079, 0.8435161051196824, 0.9509561998766194, 0.7481349911190053]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.028602</td>\n",
       "      <td>0.854029</td>\n",
       "      <td>0.905663</td>\n",
       "      <td>0.988737</td>\n",
       "      <td>[0.9976843221587227, 0.9451951722945836, 0.9393695835243202, 0.546814760418579, 0.9952162370065579, 0.8468832490303932, 0.8844753866450495, 0.676595744680851]</td>\n",
       "      <td>[0.9988679215010445, 0.9667921624095183, 0.9740426830077062, 0.6815789473684211, 0.9981039772673633, 0.9155009286050201, 0.9535437658509837, 0.7568738898756661]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.028592</td>\n",
       "      <td>0.856876</td>\n",
       "      <td>0.906539</td>\n",
       "      <td>0.988847</td>\n",
       "      <td>[0.9977235514859129, 0.9459394334314155, 0.9402583457435589, 0.5445146845779006, 0.9950504587535721, 0.8498595809030028, 0.8854099833183626, 0.6962545868657471]</td>\n",
       "      <td>[0.9989547503646576, 0.9662548187591868, 0.9756210788800022, 0.6757927427263811, 0.9985235114748836, 0.9093633675283165, 0.9459181575159367, 0.7818827708703375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>0.856872</td>\n",
       "      <td>0.909025</td>\n",
       "      <td>0.988975</td>\n",
       "      <td>[0.9977415389024756, 0.9483946344156982, 0.9413082974821315, 0.5476502674918703, 0.9953576437640494, 0.8473290801056981, 0.8871367824238129, 0.6900581101566448]</td>\n",
       "      <td>[0.9987985527893537, 0.9764990154477633, 0.9712440146963246, 0.6826740764955868, 0.9977240930074486, 0.9252492886008752, 0.9438104050997327, 0.7761989342806395]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.856870</td>\n",
       "      <td>0.906492</td>\n",
       "      <td>0.988923</td>\n",
       "      <td>[0.997732991781447, 0.9482501773866491, 0.9410842332243766, 0.5421956004833491, 0.9952079281121458, 0.8484137889831521, 0.8873904466220368, 0.6946880162663617]</td>\n",
       "      <td>[0.9987626886935135, 0.9775633025487422, 0.9725107105434192, 0.6820693036940176, 0.9983256393554052, 0.9055373554285532, 0.9404003015970936, 0.7767673179396092]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>0.857975</td>\n",
       "      <td>0.908963</td>\n",
       "      <td>0.988948</td>\n",
       "      <td>[0.9977293039989139, 0.9480754466837348, 0.9413156507954795, 0.5462841735478268, 0.9951832456002647, 0.8488920390957249, 0.8875704105319546, 0.6987507807620237]</td>\n",
       "      <td>[0.9987957214133663, 0.9718154588568101, 0.9753447490704266, 0.6888198757763975, 0.9982565351610071, 0.9069003722390939, 0.9369559256974432, 0.7948134991119006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.028079</td>\n",
       "      <td>0.858029</td>\n",
       "      <td>0.908479</td>\n",
       "      <td>0.988994</td>\n",
       "      <td>[0.997739165933247, 0.9483984262870844, 0.9415706233520933, 0.5448273158493081, 0.9952052344864715, 0.8494602084865777, 0.8884692971838941, 0.6985618771288004]</td>\n",
       "      <td>[0.9987891148693957, 0.9735800260698338, 0.9741664787623961, 0.6814972213141549, 0.9982323675738952, 0.9112923152952804, 0.943416272534101, 0.7868561278863233]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "train_single_model(\"nvidia/mit-b0\", output_dir=\"nvidia-mit-b0-training\",\n",
    "                   save_dir=\"nvidia-mit-b0-finetuned\",\n",
    "                   num_epochs=150, batch_size=60, learning_rate=1e-3,\n",
    "                   save_steps=20, eval_steps=40, logging_steps=40,\n",
    "                   warmup_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491f98b-a67c-4d34-b72d-a943855ab470",
   "metadata": {},
   "source": [
    "After +/- 2 and a half hour of training, the accuracy values reached the following values:\n",
    "\n",
    "* Label 0 'above ILM': 0.9987891148693957\n",
    "* Label 1 'ILM-IPL/INL': 0.9735800260698338\n",
    "* Label 2 'IPL/INL-RPE': 0.9741664787623961\n",
    "* Label 3 'RPE-BM': 0.6814972213141549\n",
    "* Label 4 'under BM': 0.9982323675738952\n",
    "* Label 5 'PED': 0.9112923152952804\n",
    "* Label 6 'SRF': 0.943416272534101\n",
    "* Label 7 'IRF': 0.7868561278863233\n",
    "\n",
    "Mainly label 3 and label 7 have a much lower accuracy. As I'm no ophthalmologist and can't even claim basic understanding of the subject, I have no idea if this is to be expected or not. Later I'll test with more advanced segmentation models to check if it might fix this issue. Possibly the test set isn't the best selection, or maybe the manual labeling of the dataset has been done by multiple specialists that do not fully agree and have been applying labels slightly differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14a034-9b06-4ada-aaa0-f0ac7975460f",
   "metadata": {},
   "source": [
    "The model has not yet been uploaded to HuggingFace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
