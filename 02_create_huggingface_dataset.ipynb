{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7438ccd0-71d9-40d2-ac5b-7d95f9d9a1fc",
   "metadata": {},
   "source": [
    "# Fine-tuning of a model for segmentation of retinal optical coherence tomography images (AROI)\n",
    "\n",
    "For more info, check the README.md file.\n",
    "\n",
    "This notebook creates a huggingface dataset, based on the patient_images list which was created in 01_load_patient_images.ipynb.\n",
    "\n",
    "## Citations\n",
    "\n",
    "Information about the dataset can be found in the following publications:\n",
    "\n",
    "M. Melinščak, M. Radmilović, Z. Vatavuk, and S. Lončarić, \"Annotated retinal optical coherence tomography images (AROI) database for joint retinal layer and fluid segmentation,\" Automatika, vol. 62, no. 3, pp. 375-385, Jul. 2021. doi: 10.1080/00051144.2021.1973298\n",
    "\n",
    "M. Melinščak, M. Radmilović, Z. Vatavuk, and S. Lončarić, \"AROI: Annotated Retinal OCT Images database,\" in 2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO), Sep. 2021, pp. 400-405.\n",
    "\n",
    "M. Melinščak, \"Attention-based U-net: Joint segmentation of layers and fluids from retinal OCT images,\" in 2023 46th International Convention on Information, Communication and Electronic Technology (MIPRO), Sep. 2021, pp. 391-396."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b0b8d-d316-446a-b7e8-3394919464a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run 01_load_patient_images.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7f618-9e01-4d0f-9ea0-f0679fb02810",
   "metadata": {},
   "source": [
    "Now create a HuggingFace dataset. We're not using the colour_mask images as we don't need them for fine-tuning a segmentation model.\n",
    "\n",
    "The DatasetInfo lacks some data, but that doesn't matter as I'm not planning to upload the dataset to HuggingFace Hub: that's not up to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae2cb23-75cb-4c62-a873-64d6aa3c7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the dataset, with paths for the images...\n",
      "Converting the raw image paths into images...\n",
      "Converting the labeled image paths into images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b935504d514520b14d2db64820d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_huggingface_dataset(patient_images: List[PatientImage]):\n",
    "    images: List[str] = []\n",
    "    segmentation_maps: List[str] = []\n",
    "    widths: List[int] = []\n",
    "    heights: List[int] = []\n",
    "    patient_numbers: List[int] = []\n",
    "    image_numbers: List[int] = []\n",
    "\n",
    "    pi: PatientImage\n",
    "    for pi in patient_images:\n",
    "        images.append(pi.get_raw_image_as_rgb().as_posix())\n",
    "        segmentation_maps.append(pi.number_mask_path.as_posix())\n",
    "        widths.append(PatientImage.width)\n",
    "        heights.append(PatientImage.height)\n",
    "        patient_numbers.append(pi.patient_number)\n",
    "        image_numbers.append(pi.image_number)\n",
    "\n",
    "\n",
    "    features: datasets.Features = datasets.Features({\n",
    "            'image': datasets.Value(dtype='string'),\n",
    "            'label': datasets.Value(dtype='string'),\n",
    "            'width': datasets.Value(dtype='int16'),\n",
    "            'height': datasets.Value(dtype='int16'),\n",
    "            'patient_number': datasets.Value(dtype='int16'),\n",
    "            'image_number': datasets.Value(dtype='int16'),\n",
    "        })\n",
    "    info: datasets.DatasetInfo = datasets.DatasetInfo(description=\"AROI\", citation=\"\", homepage=\"\", license=\"\", dataset_name=\"AROI\", version=\"0.0.1\", features=features)\n",
    "\n",
    "    print(\"Creating the dataset, with paths for the images...\")\n",
    "    ds = datasets.Dataset.from_dict(\n",
    "        mapping={\n",
    "            'image': images,\n",
    "            'label': segmentation_maps,\n",
    "            'width': widths,\n",
    "            'height': heights,\n",
    "            'patient_number': patient_numbers,\n",
    "            'image_number': image_numbers},\n",
    "        features=features,\n",
    "        info=info\n",
    "    )\n",
    "\n",
    "    print(\"Converting the raw image paths into images...\")\n",
    "    ds = ds.cast_column(\"image\", datasets.Image())\n",
    "    print(\"Converting the labeled image paths into images...\")\n",
    "    ds = ds.cast_column(\"label\", datasets.Image())\n",
    "    return ds\n",
    "\n",
    "complete_dataset = create_huggingface_dataset(patient_images)\n",
    "complete_dataset_path: str = \"hf_aroi_dataset_complete\"\n",
    "complete_dataset.save_to_disk(complete_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874f1d9-0908-409e-8434-7fd4c2d585a0",
   "metadata": {},
   "source": [
    "Let's check the contents of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc52044-f981-4125-a81b-d4b0e0f2e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset:\n",
      "Dataset({\n",
      "    features: ['image', 'label', 'width', 'height', 'patient_number', 'image_number'],\n",
      "    num_rows: 1137\n",
      "})\n",
      "\n",
      "First entry:\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x1024 at 0x7FFC51CE1FD0>, 'label': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x1024 at 0x7FFC51CE2E90>, 'width': 512, 'height': 1024, 'patient_number': 9, 'image_number': 106}\n",
      "\n",
      "First entry key and value pairs:\n",
      "Key: image, value: <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x1024 at 0x7FFC51CE3650>\n",
      "Key: label, value: <PIL.PngImagePlugin.PngImageFile image mode=L size=512x1024 at 0x7FFC51CE1210>\n",
      "Key: width, value: 512\n",
      "Key: height, value: 1024\n",
      "Key: patient_number, value: 9\n",
      "Key: image_number, value: 106\n",
      "\n",
      "HuggingFace features:\n",
      "{'image': Image(decode=True, id=None), 'label': Image(decode=True, id=None), 'width': Value(dtype='int16', id=None), 'height': Value(dtype='int16', id=None), 'patient_number': Value(dtype='int16', id=None), 'image_number': Value(dtype='int16', id=None)}\n",
      "\n",
      "HuggingFace features as key and value pairs:\n",
      "Key: image, value: Image(decode=True, id=None)\n",
      "Key: label, value: Image(decode=True, id=None)\n",
      "Key: width, value: Value(dtype='int16', id=None)\n",
      "Key: height, value: Value(dtype='int16', id=None)\n",
      "Key: patient_number, value: Value(dtype='int16', id=None)\n",
      "Key: image_number, value: Value(dtype='int16', id=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset:\")\n",
    "print(complete_dataset)\n",
    "print(\"\\nFirst entry:\")\n",
    "print(complete_dataset[0])\n",
    "print(\"\\nFirst entry key and value pairs:\")\n",
    "for k,v in complete_dataset[0].items():\n",
    "    print(f\"Key: {k}, value: {v}\")\n",
    "print(\"\\nHuggingFace features:\")\n",
    "print(complete_dataset.features)\n",
    "print(\"\\nHuggingFace features as key and value pairs:\")\n",
    "for k,v in complete_dataset.features.items():\n",
    "    print(f\"Key: {k}, value: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b0051-f0cc-4b81-9f2e-be0cd2a8c2d3",
   "metadata": {},
   "source": [
    "The following code is untested. It should in theory allow anyone to upload the dataset to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad7ed4d1-355c-4eb4-ac0d-342c2d21d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used on the 'complete_dataset' or the 'split_dataset' which is defined later in this notebook\n",
    "def example_upload_to_huggingface_hub(ds: datasets.Dataset):\n",
    "    import huggingface_hub\n",
    "    hf_token: typing.Optional[str] = huggingface_hub.HfFolder.get_token()\n",
    "    if hf_token is None:\n",
    "        print(\"You first need to login to use the HugginFace hub\")\n",
    "    else:\n",
    "        dataset.push_to_hub('<username>/retinal_optical_coherence_tomography_images_complete', private=False, token=hf_token)\n",
    "    # You'll need to write a README.md with some metadata at the top. An example:\n",
    "    # https://github.com/DriesVerachtert/basic_shapes_object_detection_dataset/blob/main/README.md\n",
    "    readme_file: str = \"SOME_README.md\"\n",
    "    readme_contents: str\n",
    "    with open(readme_file, 'r') as file:\n",
    "        readme_contents = file.read()\n",
    "    card = huggingface_hub.repocard.RepoCard(readme_contents)\n",
    "    card.push_to_hub('<username>/retinal_optical_coherence_tomography_images_complete', token=hf_token, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7ac3d-ab42-463c-a113-c8e8865b2c07",
   "metadata": {},
   "source": [
    "Dictionaries that map the number values of the labels to their descriptions and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a60bfee-eea8-4f0e-8bb0-e8c0592ce06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label: Dict[int,str] = {v: k for v, k in enumerate(annotations_short)}\n",
    "label2id: Dict[str,int] = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d9bdf-7dc9-473c-9058-a96c4e4bfc92",
   "metadata": {},
   "source": [
    "When creating a train set and a test set, we have to make sure they contain images that contain all the labels: we shouldn't make a test set with images which do not contain label '5' for example. For each label, let's check how many images contain that label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e8c233-be01-4c51-8f52-b803ee40ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137 images use class 0\n",
      "1137 images use class 1\n",
      "1137 images use class 2\n",
      "1137 images use class 3\n",
      "1137 images use class 4\n",
      "1014 images use class 5\n",
      "649 images use class 6\n",
      "228 images use class 7\n",
      "All labels are represented\n"
     ]
    }
   ],
   "source": [
    "def check_distribution_of_labels_across_dataset(ds: datasets.Dataset):\n",
    "    num_images_per_class: Dict[int,int] = defaultdict(int)\n",
    "\n",
    "    for ds_entry in ds:\n",
    "        img: Dict = cast(Dict, ds_entry)\n",
    "        unique_label_ids: np.ndarray = np.unique(img['label'])\n",
    "        \n",
    "        for np_id in unique_label_ids:\n",
    "            id: int = int(np_id)\n",
    "            num_images_per_class[id] = num_images_per_class[id] + 1\n",
    "\n",
    "    for k,v in num_images_per_class.items():\n",
    "        print(f\"{v} images use class {k}\")\n",
    "    if len(num_images_per_class) == len(id2label):\n",
    "        print(\"All labels are represented\")\n",
    "\n",
    "\n",
    "check_distribution_of_labels_across_dataset(complete_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b00a5b-003c-4798-a5d4-80b9b3e62ec7",
   "metadata": {},
   "source": [
    "Only +/- 1 out of 5 images contains label 7 => we can't simply use some random images as a test set.\n",
    "\n",
    "The following method selects a set of images, that contains at least 10 images using each of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "856453f5-1b2f-4910-9544-956877f624d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected indexes for a test dataset: [228, 51, 563, 501, 457, 285, 209, 1116, 178, 864, 65, 61, 191, 859, 865, 318, 704, 928, 727, 664, 292, 877]\n"
     ]
    }
   ],
   "source": [
    "def select_indexes_for_test_dataset(ds: datasets.Dataset, min_images_per_label: int = 10) -> List[int]:\n",
    "    \"\"\" Select indexes of entries within this dataset that could be used as a test dataset\n",
    "    Make sure that at least each label is represented by min_images_per_label images\n",
    "    Note: uses \"\"\"\n",
    "    def enough_images_selected(num_images_per_label: Dict[int,int], num_classes: int) -> bool:\n",
    "        for id in range(0,num_classes):\n",
    "            if num_images_per_label[id] < min_images_per_label:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def check_if_contains_needed_labels(num_images_per_label: Dict[int,int], labels_of_image: np.ndarray) -> bool:\n",
    "        \"\"\" some labels might not have enough images yet within num_images_per_label\n",
    "        Check if the labels within the randomly selected image, contains labels that are not yet enough represented \"\"\"\n",
    "        for np_id in labels_of_image:\n",
    "            id: int = int(np_id)\n",
    "            if num_images_per_label[id] < min_images_per_label:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    num_images_per_label: Dict[int,int] = defaultdict(int)\n",
    "    selected_indexes: List[int] = []\n",
    "\n",
    "    while not enough_images_selected(num_images_per_label, len(id2label)):\n",
    "        # select a random entry in the dataset\n",
    "        random_index: int = random.randint(0, len(ds) - 1)\n",
    "        if random_index in selected_indexes:\n",
    "            # this index was already selected\n",
    "            continue\n",
    "        if 'label' not in ds[random_index]:\n",
    "            print(f\"no 'label' key for entry at index {random_index}\")\n",
    "            raise ValueError(\"This method can only be used before adding the transform method which uses the feature extractor\")\n",
    "        labels_of_image: np.ndarray = np.unique(ds[random_index]['label'])\n",
    "        if check_if_contains_needed_labels(num_images_per_label, labels_of_image):\n",
    "            # Let's add this image\n",
    "            selected_indexes.append(random_index)\n",
    "            for np_id in labels_of_image:\n",
    "                id: int = int(np_id)\n",
    "                num_images_per_label[id] = num_images_per_label[id] + 1\n",
    "    return selected_indexes\n",
    "\n",
    "\n",
    "indexes_for_test_dataset = select_indexes_for_test_dataset(complete_dataset, min_images_per_label=10)\n",
    "print(f\"Selected indexes for a test dataset: {indexes_for_test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ee607-e992-497f-881d-2864aefdbacd",
   "metadata": {},
   "source": [
    "Let's make a train and a test dataset from the complete dataset:\n",
    "* The test dataset will contain the small number of images that were selected above.\n",
    "* The train dataset will contain all remaining images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c725202-d34e-4e72-a145-ba71f752722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_test dataset has 22 entries with the following distribution of label usage:\n",
      "22 images use class 0\n",
      "22 images use class 1\n",
      "22 images use class 2\n",
      "22 images use class 3\n",
      "22 images use class 4\n",
      "22 images use class 5\n",
      "14 images use class 6\n",
      "10 images use class 7\n",
      "All labels are represented\n",
      "ds_train dataset has 1115 entries with the following distribution of label usage:\n",
      "1115 images use class 0\n",
      "1115 images use class 1\n",
      "1115 images use class 2\n",
      "1115 images use class 3\n",
      "1115 images use class 4\n",
      "992 images use class 5\n",
      "635 images use class 6\n",
      "218 images use class 7\n",
      "All labels are represented\n"
     ]
    }
   ],
   "source": [
    "test_dataset: datasets.Dataset = complete_dataset.select(indexes_for_test_dataset)\n",
    "print(f\"ds_test dataset has {len(test_dataset)} entries with the following distribution of label usage:\")\n",
    "check_distribution_of_labels_across_dataset(test_dataset)\n",
    "\n",
    "train_dataset: datasets.Dataset = complete_dataset.select([x for x in range(0,len(complete_dataset)) if x not in indexes_for_test_dataset])\n",
    "print(f\"ds_train dataset has {len(train_dataset)} entries with the following distribution of label usage:\")\n",
    "check_distribution_of_labels_across_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f09ee-2f7d-4c5d-bc27-b510efc7197b",
   "metadata": {},
   "source": [
    "Create a dataset containing the two splits 'train' and test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df1ad569-a50e-412b-bc77-71e8710fa74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset: datasets.DatasetDict = datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f43b6b-96bf-4173-8747-15cb80a71f13",
   "metadata": {},
   "source": [
    "Store also this dataset on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50caa521-2d44-42c1-8ae9-69ae74b84e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30eb907b29743339998a9f18ef796a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b4613eb102403d92b0a56fd1c0cbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_dataset_path: str = \"hf_aroi_dataset_split\"\n",
    "split_dataset.save_to_disk(split_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192593dc-e8ed-42a6-bea4-70ca8edaf726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
